{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-auth in c:\\users\\acer\\anaconda3\\lib\\site-packages (2.18.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in c:\\users\\acer\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: google-auth-httplib2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from google-auth) (1.26.14)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from google-auth) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from google-auth) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from google-auth) (5.3.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from google-auth-oauthlib) (1.3.1)\n",
      "Requirement already satisfied: httplib2>=0.15.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from google-auth-httplib2) (0.22.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from httplib2>=0.15.0->google-auth-httplib2) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install google-auth google-auth-oauthlib google-auth-httplib2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind': 'youtube#channelListResponse', 'etag': 'Zg338v7rsjxjOgPlEvnPGlDADxw', 'pageInfo': {'totalResults': 1, 'resultsPerPage': 5}, 'items': [{'kind': 'youtube#channel', 'etag': 'fLJSl4p9bZ4Vtk5RKT6uCa16IDg', 'id': 'UCCezIgC97PvUuR4_gbFUs5g', 'statistics': {'viewCount': '87799003', 'subscriberCount': '1170000', 'hiddenSubscriberCount': False, 'videoCount': '232'}}]}\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "api_key = 'AIzaSyBF4Ehu55oIyp3Qzj2gNhTxP3skbMOMGBo'\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "request = youtube.channels().list(\n",
    "        part='statistics',\n",
    "        forUsername='schafer5'\n",
    "    )\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_threads(channelID, to_csv):\n",
    "\n",
    "    comments_list = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='id,snippet',\n",
    "        videoId=channelID,\n",
    "        order='relevance'\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    comments_list.extend(process_comments(response['items']))\n",
    "\n",
    "    while response.get('nextPageToken', None):\n",
    "        request = youtube.commentThreads().list(\n",
    "            part='id,snippet',\n",
    "            videoId=channelID,\n",
    "            order='relevance',\n",
    "            pageToken=response['nextPageToken']\n",
    "        )\n",
    "        response = request.execute()\n",
    "        comments_list.extend(process_comments(response['items']))\n",
    "        print(f\"\\rFetched {len(comments_list)} comments...\", end='', flush=True)\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"Finished fetching {len(comments_list)} comments\")\n",
    "\n",
    "    if to_csv:\n",
    "        make_csv(comments_list, channelID)\n",
    "\n",
    "    return comments_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video title: Roasting A Convicted Murderer | Andrew Schulz | Stand Up Comedy\n",
      "View count: 8908499\n",
      "Like count: 199612\n",
      "Comment count: 4845\n",
      "Subscriber count: 2750000\n",
      "___________________________________________________\n",
      "Fetched 1295 comments...\n",
      "Finished fetching 1295 comments\n",
      "USING BLAZING TEXT MODEL\n",
      "No.of positive comments : 1049\n",
      "No.of negative comments : 163\n",
      "No.of neutral comments : 83\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "Total score for the video 'Roasting A Convicted Murderer | Andrew Schulz | Stand Up Comedy': 0.35672930101544403\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import googleapiclient.discovery\n",
    "\n",
    "# load_dotenv()\n",
    "API = \"AIzaSyBF4Ehu55oIyp3Qzj2gNhTxP3skbMOMGBo\"\n",
    "video_id = \"KdlawuL19U4\"\n",
    "Comment_thresh = 4000\n",
    "\n",
    "def main():\n",
    "\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    DEVELOPER_KEY = API\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n",
    "\n",
    "    video_response = youtube.videos().list(part='statistics', id=video_id).execute()\n",
    "\n",
    "\n",
    "    video_title, like_count, view_count = vid_info(video_id, youtube)\n",
    "\n",
    "    comment_threads(video_id, True)\n",
    "\n",
    "    pos_comments,neu_comments,neg_comments = predict_scores(video_id)\n",
    "\n",
    "    print(\"____________________________________________\")\n",
    "    print(\"____________________________________________\")\n",
    "    \n",
    "    # total = (pos_comments/(pos_comments+neu_comments+neg_comments))*0.6 + (like_count/view_count)*0.4\n",
    "    total_comments = pos_comments + neu_comments + neg_comments\n",
    "    sentiment_score = (pos_comments / total_comments) * 0.4\n",
    "    view_score = (int(view_count) / 100000000) * 0.3\n",
    "    like_score = (int(like_count) / 10000000) * 0.3\n",
    "\n",
    "    total = sentiment_score + view_score + like_score \n",
    "    print(f\"Total score for the video '{video_title}': {total}\")\n",
    "\n",
    "    # print(f\"Total score for the video '\"+ video_title +f\"' {total}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vid_info(video_id, youtube):\n",
    "    video_response = youtube.videos().list(part='snippet,statistics', id=video_id).execute()\n",
    "\n",
    "    # Extract video details and statistics from the response\n",
    "    video_details = video_response['items'][0]['snippet']\n",
    "    statistics = video_response['items'][0]['statistics']\n",
    "\n",
    "    # Get view count, like count, comment count, and title\n",
    "    view_count = statistics.get('viewCount', 'N/A')\n",
    "    like_count = statistics.get('likeCount', 'N/A')\n",
    "    comment_count = statistics.get('commentCount', 'N/A')\n",
    "    video_title = video_details.get('title', 'Title not found')\n",
    "\n",
    "    # Make a request to the API to retrieve channel information\n",
    "    channel_id = video_details['channelId']\n",
    "\n",
    "    channel_response = youtube.channels().list(part='statistics', id=channel_id).execute()\n",
    "    channel_statistics = channel_response['items'][0]['statistics']\n",
    "\n",
    "    # Get subscriber count\n",
    "    subscriber_count = channel_statistics.get('subscriberCount', 'N/A')\n",
    "\n",
    "    print(f\"Video title: {video_title}\")\n",
    "    print(f\"View count: {view_count}\")\n",
    "    print(f\"Like count: {like_count}\")\n",
    "    print(f\"Comment count: {comment_count}\")\n",
    "    print(f\"Subscriber count: {subscriber_count}\")\n",
    "    print(\"___________________________________________________\")\n",
    "    \n",
    "    return video_title, like_count, view_count\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_comments(response_items):\n",
    "    comments = []\n",
    "    for res in response_items:\n",
    "        #handles replies\n",
    "        if 'replies' in res.keys():\n",
    "            for reply in res['replies']['comments']:\n",
    "                comment = reply[\"snippet\"]\n",
    "                comment['commentId'] = reply['id']\n",
    "                comments.append(comment)\n",
    "                \n",
    "            \n",
    "        #handles non-replies\n",
    "        else:\n",
    "            comment = {}\n",
    "            comment['snippet'] = res['snippet']['topLevelComment']['snippet']\n",
    "            comment['snippet']['parentId'] = None\n",
    "            comment['snippet']['commentId'] = res['snippet']['topLevelComment']['id']\n",
    "            comments.append(comment['snippet'])\n",
    "\n",
    "    # print(comments)\n",
    "    return comments\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def make_csv(comments, channelID = None):\n",
    "    header = comments[0].keys()\n",
    "    if channelID:\n",
    "        filename = f'comments_{channelID}.csv'\n",
    "    else:\n",
    "        filename = 'comments.csv'\n",
    "\n",
    "    with open(filename, 'w', encoding = 'utf8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(comments)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "def predict_scores(vid_id):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    csv_file_path = f'comments_{vid_id}.csv' \n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract the content under the 'textOriginal' column\n",
    "    text_original_list = data['textOriginal'].tolist()\n",
    "    cleaned = []\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    \n",
    "    for i in text_original_list:\n",
    "        if isinstance(i, str):  # Check if 'i' is a string\n",
    "            corpus =[]\n",
    "            dialog = re.sub('[^a-zA-Z,]',' ', string=i) # Cleaning special character from the dialog/script\n",
    "            dialog = dialog.lower()\n",
    "            script = dialog.split()\n",
    "            script = [wordnet.lemmatize(word) for word in script if not word in set(stopwords.words('english'))]\n",
    "            final = ' '.join(script)\n",
    "            cleaned .append(final)\n",
    "    \n",
    "\n",
    "    sentiment = []\n",
    "    sentiment_vader = []\n",
    "    sentiment = blazingtext_model(cleaned)\n",
    "    \n",
    "    for i in cleaned:\n",
    "        sentiment_vader.append(classify_sentiment(i))\n",
    "\n",
    "    value_counts = Counter(sentiment)\n",
    "    pos_comments,neu_comments,neg_comments = 0,0,0\n",
    "    \n",
    "    # blazing text\n",
    "    print(\"USING BLAZING TEXT MODEL\")\n",
    "    for value, count in value_counts.items():\n",
    "        if(value==1):\n",
    "            print(f\"No.of positive comments : {count}\")\n",
    "            pos_comments = count\n",
    "        elif(value==0):\n",
    "            print(f\"No.of neutral comments : {count}\")\n",
    "            neu_comments = count\n",
    "        elif(value==-1):\n",
    "            print(f\"No.of negative comments : {count}\")\n",
    "            neg_comments = count\n",
    "\n",
    "    \n",
    "    #vader\n",
    "    \n",
    "    \n",
    "    return pos_comments,neu_comments,neg_comments\n",
    "\n",
    "    # print(\"USING VADER\")\n",
    "    # value_counts_vader = Counter(sentiment_vader)\n",
    "\n",
    "    # for value, count in value_counts_vader.items():\n",
    "    #     if(value==1):\n",
    "    #         print(f\"No.of positive comments : {count}\")\n",
    "    #     elif(value==0):\n",
    "    #         print(f\"No.of neutral comments : {count}\")\n",
    "    #     elif(value==-1):\n",
    "    #         print(f\"No.of negative comments : {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# instances = ['This product is great !', 'OK , but not great', 'This is not the right product .']\n",
    "\n",
    "def blazingtext_model(instances):\n",
    "    # Convert instances to a dictionary with the appropriate key\n",
    "    payload = {\"instances\": instances}\n",
    "\n",
    "    payload_json = json.dumps(payload)\n",
    "\n",
    "    endpoint_name = 'blazingtext-2023-08-03-09-30-40-657'\n",
    "\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        ContentType='application/json',\n",
    "        Body=payload_json.encode()\n",
    "        )\n",
    "\n",
    "    sess = sagemaker.Session()\n",
    "\n",
    "    endpoint_name = 'blazingtext-2023-08-03-09-30-40-657'\n",
    "\n",
    "    preds = response['Body'].read().decode('utf-8')\n",
    "    # print(predictions)\n",
    "\n",
    "    parsed_response = json.loads(preds)\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    for prediction in parsed_response:\n",
    "        predicted_class = int(prediction['label'][0].split('__label__')[-1])\n",
    "        pred_list.append(predicted_class)\n",
    "    \n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download VADER lexicon data\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    scores = sentiment.polarity_scores(text)\n",
    "    if scores['pos'] > scores['neg'] and scores['pos'] > scores['neu']:\n",
    "        return 1\n",
    "    elif scores['neg'] > scores['pos'] and scores['neg'] > scores['neu']:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    sentiment_labels = [classify_sentiment(text) for text in texts]\n",
    "\n",
    "    return sentiment_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
